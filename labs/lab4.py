import streamlit as st
from openai import OpenAI
import chromadb
from chromadb.utils import embedding_functions
import zipfile
from pypdf import PdfReader
from io import BytesIO
import os
import tiktoken


def create_lab4_vectordb():
    """
    Construct a ChromaDB collection named "Lab4Collection" with PDF documents.
    Uses OpenAI embeddings model (text-embedding-3-small).
    Stores the collection in st.session_state.Lab4_VectorDB to avoid recreating it.
    """
    # Check if vector database already exists in session state
    if "Lab4_VectorDB" in st.session_state:
        return st.session_state.Lab4_VectorDB
    
    # Initialize OpenAI client
    if "client" not in st.session_state:
        api_key = st.secrets["openai_api_key"]
        st.session_state.client = OpenAI(api_key=api_key)
    
    client = st.session_state.client
    
    # Create OpenAI embedding function
    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=st.secrets["openai_api_key"],
        model_name="text-embedding-3-small"
    )
    
    # Initialize ChromaDB client (persistent storage)
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    
    # Create or get the collection
    try:
        collection = chroma_client.get_collection(
            name="Lab4Collection",
            embedding_function=openai_ef
        )
        # Collection exists, check if it has documents
        if collection.count() > 0:
            st.session_state.Lab4_VectorDB = collection
            return collection
    except Exception:
        # Collection doesn't exist, create it
        collection = chroma_client.create_collection(
            name="Lab4Collection",
            embedding_function=openai_ef
        )
    
    # Path to the zip file: try project root, data/, script-relative, then ~/Downloads
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(script_dir, ".."))
    possible_paths = [
        "Lab-04-Data.zip",
        "data/Lab-04-Data.zip",
        os.path.join(project_root, "Lab-04-Data.zip"),
        os.path.join(project_root, "data", "Lab-04-Data.zip"),
        os.path.expanduser("~/Downloads/Lab-04-Data.zip"),
    ]
    zip_path = None
    for p in possible_paths:
        if os.path.isfile(p):
            zip_path = p
            break
    if zip_path is None:
        st.error(
            "**Lab-04-Data.zip** not found. "
            "For **Streamlit Cloud**: add the zip to the repo (project root or `data/` folder). "
            "For **local**: put it in your project root, in a `data/` folder, or in **Downloads**."
        )
        return None
    
    # List of PDF files to process
    pdf_files = [
        "IST 488 Syllabus - Building Human-Centered AI Applications.pdf",
        "IST 314 Syllabus - Interacting with AI.pdf",
        "IST 343 Syllabus - Data in Society.pdf",
        "IST 256 Syllabus - Intro to Python for the Information Profession.pdf",
        "IST 387 Syllabus - Introduction to Applied Data Science.pdf",
        "IST 418 Syllabus - Big Data Analytics.pdf",
        "IST 195 Syllabus - Information Technologies.pdf"
    ]
    
    # Extract and process PDFs from zip file
    documents = []
    metadatas = []
    ids = []
    
    try:
        zip_ref = zipfile.ZipFile(zip_path, 'r')
    except FileNotFoundError:
        st.error(f"Zip file not found: {zip_path}")
        return None
    except OSError as e:
        st.error(f"Cannot open zip file: {e}")
        return None

    with zip_ref:
        for pdf_filename in pdf_files:
            # Construct the full path within the zip
            zip_internal_path = f"Lab-04-Data/{pdf_filename}"
            
            try:
                # Read PDF from zip
                pdf_bytes = zip_ref.read(zip_internal_path)
                pdf_reader = PdfReader(BytesIO(pdf_bytes))
                
                # Extract text from all pages
                text_content = ""
                for page in pdf_reader.pages:
                    text_content += page.extract_text() + "\n"
                
                # Clean up text (remove excessive whitespace)
                text_content = " ".join(text_content.split())
                
                if text_content.strip():  # Only add if text was extracted
                    documents.append(text_content)
                    metadatas.append({
                        "filename": pdf_filename,
                        "source": "Lab-04-Data"
                    })
                    ids.append(pdf_filename)  # Use filename as unique ID
                    
            except KeyError:
                st.warning(f"PDF file not found in zip: {pdf_filename}")
                continue
            except Exception as e:
                st.error(f"Error processing {pdf_filename}: {str(e)}")
                continue
    
    # Add documents to ChromaDB collection
    if documents:
        collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        st.session_state.Lab4_VectorDB = collection
        st.success(f"Successfully created ChromaDB collection with {len(documents)} documents!")
    else:
        st.error("No documents were successfully processed from the PDF files.")
        return None
    
    return collection


# --- Lab 4 page UI: Course information chatbot (RAG) ---
st.title("Lab 4 – Course information chatbot")

# Initialize the vector DB when the page loads (only runs if not already in session_state)
vectordb = create_lab4_vectordb()

if vectordb is None:
    st.warning(
        "Vector DB could not be loaded. Check that the Lab-04-Data.zip file is available "
        "at the expected path and that the OpenAI API key is set in secrets."
    )
    st.stop()

count = vectordb.count()
st.caption(f"Vector DB ready with {count} syllabus documents. Ask questions below—answers will cite when they use course materials.")
st.write("")  # spacing

# --- Lab 3–style setup: token counting, model choice, phase, messages ---
max_tokens = 1000
openAI_model = st.sidebar.selectbox("Which Model?", ("mini", "regular"), key="lab4_model")
model_to_use = "gpt-4o-mini" if openAI_model == "mini" else "gpt-4o"

try:
    encoding = tiktoken.encoding_for_model("gpt-4o")
except Exception:
    encoding = tiktoken.get_encoding("cl100k_base")


def count_tokens(messages):
    total = 3
    for m in messages:
        total += 4
        total += len(encoding.encode(m["role"]))
        total += len(encoding.encode(m.get("content", "") or ""))
    return total


KID_FRIENDLY_SYSTEM = (
    "You explain things in a simple, friendly way so that a 10-year-old can understand. "
    "Use short sentences and everyday words. When you answer a question, give a clear answer "
    "and then ask: 'Do you want more info?' When the user wants more info, give more details "
    "on the same topic in the same simple style, then ask again: 'Do you want more info?'"
)

if "lab4_phase" not in st.session_state:
    st.session_state.lab4_phase = "ask_question"
if "lab4_last_question" not in st.session_state:
    st.session_state.lab4_last_question = ""


def is_yes(text):
    t = (text or "").strip().lower()
    return t in ("yes", "y", "yeah", "yep", "sure", "more", "please")


def is_no(text):
    t = (text or "").strip().lower()
    return t in ("no", "n", "nope", "nah", "no thanks")


if "lab4_messages" not in st.session_state:
    st.session_state.lab4_messages = [
        {"role": "assistant", "content": "What would you like to know about the syllabi? Ask me anything!"}
    ]

# Ensure OpenAI client exists for chat
if "client" not in st.session_state:
    api_key = st.secrets["openai_api_key"]
    st.session_state.client = OpenAI(api_key=api_key)
client = st.session_state.client

# Render chat history
for msg in st.session_state.lab4_messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Chat input and RAG + LLM flow
if prompt := st.chat_input("Enter a message"):
    st.session_state.lab4_messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    phase = st.session_state.lab4_phase
    last_question = st.session_state.lab4_last_question

    # ---- User said "No" → back to help ----
    if phase in ("answered_ask_more", "gave_more_ask_again") and is_no(prompt):
        reply = "Sure! What else can I help you with?"
        with st.chat_message("assistant"):
            st.markdown(reply)
        st.session_state.lab4_messages.append({"role": "assistant", "content": reply})
        st.session_state.lab4_phase = "ask_question"
        st.session_state.lab4_last_question = ""
    # ---- User said "Yes" (want more info) ----
    elif phase in ("answered_ask_more", "gave_more_ask_again") and is_yes(prompt):
        messages_for_llm = [{"role": "system", "content": KID_FRIENDLY_SYSTEM}]
        messages_for_llm.extend(st.session_state.lab4_messages[-6:])
        messages_for_llm.append({
            "role": "user",
            "content": "The user said they want more information. Give more details about what we were just talking about, in the same simple way. Then end by asking: Do you want more info?",
        })
        if count_tokens(messages_for_llm) > max_tokens:
            messages_for_llm = [messages_for_llm[0]] + messages_for_llm[-4:]
        st.caption(f"Tokens sent to LLM: {count_tokens(messages_for_llm)} / {max_tokens}")
        stream = client.chat.completions.create(
            model=model_to_use,
            messages=messages_for_llm,
            stream=True,
        )
        with st.chat_message("assistant"):
            response = st.write_stream(stream)
        st.session_state.lab4_messages.append({"role": "assistant", "content": response})
        st.session_state.lab4_phase = "gave_more_ask_again"
    # ---- New question: RAG (retrieve from vector DB) then answer ----
    else:
        if phase == "ask_question" or not (is_yes(prompt) or is_no(prompt)):
            st.session_state.lab4_last_question = prompt

        # Retrieve relevant chunks from Lab4 collection
        n_results = 3
        results = vectordb.query(
            query_texts=[prompt],
            n_results=min(n_results, vectordb.count()),
            include=["documents", "metadatas"]
        )
        context_parts = []
        if results and results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                meta = (results["metadatas"][0][i] if results["metadatas"] and results["metadatas"][0] else {}) or {}
                src = meta.get("filename", "syllabus")
                context_parts.append(f"[Source: {src}]\n{doc}")
        context_text = "\n\n---\n\n".join(context_parts) if context_parts else "(No relevant passages found.)"

        # Prompt engineering: require the bot to be clear when using RAG vs general knowledge
        system_with_context = (
            KID_FRIENDLY_SYSTEM
            + "\n\nYou have access to the following excerpts from course syllabi (retrieved by RAG). "
            "When your answer is based on these excerpts, you MUST say so clearly at the start, e.g. "
            "'Based on the course syllabi:' or 'According to the syllabus materials I have:'. "
            "When the answer is NOT in the excerpts, you MUST say so clearly, e.g. "
            "'This isn’t in the syllabi I have; from general knowledge:' or 'The syllabi don’t mention this; here’s what I know:'. "
            "Keep answers simple and kid-friendly.\n\nSyllabus excerpts (use these when they answer the question):\n"
            + context_text
        )

        messages_for_llm = [{"role": "system", "content": system_with_context}]
        messages_for_llm.extend(st.session_state.lab4_messages)
        while count_tokens(messages_for_llm) > max_tokens and len(messages_for_llm) > 3:
            if messages_for_llm[1]["role"] == "assistant":
                messages_for_llm = [messages_for_llm[0]] + messages_for_llm[3:]
            else:
                messages_for_llm = [messages_for_llm[0]] + messages_for_llm[2:]
        st.caption(f"Tokens sent to LLM: {count_tokens(messages_for_llm)} / {max_tokens}")
        stream = client.chat.completions.create(
            model=model_to_use,
            messages=messages_for_llm,
            stream=True,
        )
        with st.chat_message("assistant"):
            response = st.write_stream(stream)
        st.session_state.lab4_messages.append({"role": "assistant", "content": response})
        st.session_state.lab4_phase = "answered_ask_more"

    # Trim message history to stay under token budget
    messages = st.session_state.lab4_messages
    while count_tokens(messages) > max_tokens and len(messages) > 2:
        if messages[0]["role"] == "assistant" and len(messages) > 3:
            messages = [messages[0]] + messages[3:]
        else:
            messages = messages[2:]
    st.session_state.lab4_messages = messages